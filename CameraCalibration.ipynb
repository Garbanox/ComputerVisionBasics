{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absolute-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-magnet",
   "metadata": {},
   "source": [
    "# Camera Calibration and Undistorting Images\n",
    "In this notebook we are going to calibrate a monocular camera (i.e. estimate the camera matrix and distortion parameters) and use the calibration to undistort an image. We will start by using the standard OpenCV functions first and later implement the calibration and undistortion form scratch using numpy and scipy.\n",
    "## Camera Calibration using OpenCV\n",
    "I have printed out a [chessboard pattern](https://github.com/opencv/opencv/blob/master/doc/pattern.png) and took pictures of it using my phone's camera. For efficiency reasons I scaled down the images by a factor of 2. We will follow the [official OpenCV tutorial on camera calibration](https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html).\n",
    "<img src=\"data/IMG_1239.jpg\" alt=\"drawing\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 29 images and found 29 chessboards.\n"
     ]
    }
   ],
   "source": [
    "# Extract circle centers for all images. This might take a while...\n",
    "image_files = glob.glob('data/*.jpg')\n",
    "\n",
    "grid_size = 25 # mm\n",
    "grid_shape = (9,6)\n",
    "\n",
    "image_corners = np.zeros((0, grid_shape[0]*grid_shape[1] ,2))\n",
    "chessboard_positions = np.zeros((grid_shape[0]*grid_shape[1],3))\n",
    "chessboard_positions[:,:2] = np.mgrid[0:grid_shape[0],0:grid_shape[1]].T.reshape(-1,2)\n",
    "chessboard_positions *= grid_size\n",
    "image_shape = np.array([])\n",
    "\n",
    "for file in image_files:\n",
    "    img = cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2GRAY)\n",
    "    image_shape = img.shape\n",
    "    ret, corners = cv2.findChessboardCorners(img, grid_shape)\n",
    "    if ret:\n",
    "        image_corners = np.append(image_corners, np.array(corners).reshape(1, -1, 2), axis=0)\n",
    "    else:\n",
    "        print(f\"No chessboard detected in image {file}\")\n",
    "print(f\"Processed {len(image_files)} images and found {image_corners.shape[0]} chessboards.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-pacific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera matrix returned by OpenCV:\n",
      "[[1534.97    0.   1010.01]\n",
      " [   0.   1558.1   747.42]\n",
      " [   0.      0.      1.  ]]\n",
      "Distortion coefficients returned by OpenCV:\n",
      "k1: +2.213e-01; \tk2: -9.994e-01; \tp1: -2.081e-05; \tp2: +4.839e-05, ; \tk3: +1.396e+00\n"
     ]
    }
   ],
   "source": [
    "def calibrate_camera_openCV():\n",
    "    object_points = np.tile(chessboard_positions, (image_corners.shape[0], 1, 1))\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(object_points.astype(np.float32), image_corners.astype(np.float32), image_shape[::-1], None, None)\n",
    "    \n",
    "    n = len(rvecs)\n",
    "    transformations = np.zeros((n, 3, 4))\n",
    "    for i in range(n):\n",
    "        transformations[i,:,:3] = cv2.Rodrigues(rvecs[i])[0]\n",
    "        transformations[i,:,3] = tvecs[i].flatten()\n",
    "    \n",
    "    \n",
    "    return mtx, dist[0], transformations\n",
    "\n",
    "camera_mat_cv, dist_cv, transformations_cv = calibrate_camera_openCV()\n",
    "print(\"Camera matrix returned by OpenCV:\")\n",
    "with np.printoptions(suppress=True, precision=2):\n",
    "    print(camera_mat_cv)\n",
    "print(\"Distortion coefficients returned by OpenCV:\")\n",
    "print(f\"k1: {dist_cv[0]:+.3e}; \\tk2: {dist_cv[1]:+.3e}; \\tp1: {dist_cv[2]:+.3e}; \\tp2: {dist_cv[3]:+.3e}, ; \\tk3: {dist_cv[4]:+.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-absence",
   "metadata": {},
   "source": [
    "## Camera Calibration from Scratch\n",
    "### Estimating the Camera Matrix\n",
    "In this section we are going to implement the camera calibration procedure inspired by [Zhang '98](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf). \n",
    "We write down the perspective projection equation for a planar grid. The origin of the world is therefore defined by the chessboardd pattern. Note that all z-coordinates are zero because all world points lie on the plane. \n",
    "\n",
    "$$ \n",
    "\\lambda \\left( \\begin{array}{r} u \\\\ v \\\\ 1 \\end{array}\\right) = K \n",
    "\\left[ \\begin{array}{c c} R_{cw} &  T_{cw} \\end{array}\\right] \n",
    "\\left( \\begin{array}{c} X_w \\\\ Y_w \\\\ 0 \\\\ 1\\end{array}\\right) = \n",
    "\\underbrace{K \\left[ \\begin{array}{c c } r_1 & r_2 & t\\end{array}\\right]}_{H} \n",
    "\\left( \\begin{array}{c} X_w \\\\ Y_w \\\\ 1\\end{array}\\right)\n",
    "$$\n",
    "The matrix $H$ is the same for all corners in one image and can be estimated using for example the **direct linear transform (DLT)**. This however assumes that all elements in $H$ are independent which in in general not the case. We will only use the DLT only as a first guess and refine the estimation later on using all constraints.\n",
    "\n",
    "$$ \n",
    "\\lambda \\left( \\begin{array}{r} u \\\\ v \\\\ 1 \\end{array}\\right) = \n",
    "\\left( \\begin{array}{ccc}\n",
    "    h_{11} & h_{12} & h_{13}\\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33}\n",
    "\\end{array}\\right) \n",
    "\\left( \\begin{array}{c} X_w \\\\ Y_w \\\\ 1\\end{array}\\right) = \n",
    "\\left( \\begin{array}{c}h_1^T \\\\ h_2^T \\\\ h_3^T\\end{array}\\right) P \n",
    "\\Rightarrow \n",
    "\\left\\{\n",
    "\\begin{array}{l l}\n",
    "    u = \\frac{\\lambda u}{\\lambda} = \\frac{h_1^T \\cdot P}{h_3^T \\cdot P} \\\\ \n",
    "    v = \\frac{\\lambda v}{\\lambda} = \\frac{h_2^T \\cdot P}{h_3^T \\cdot P}\n",
    "\\end{array}\\right.\n",
    "\\Rightarrow \n",
    "\\left\\{\n",
    "\\begin{array}{l l}\n",
    "    (h_1^T - u h_3^T)\\cdot P=0 \\\\ \n",
    "    (h_2^T - v h_3^T)\\cdot P=0 \n",
    "\\end{array}\\right.\n",
    "\\Rightarrow \n",
    "\\left( \\begin{array}{ccc}\n",
    "P^T & 0^T & -uP^T \\\\0^T & P^T & -vP^T\n",
    "\\end{array}\\right) \n",
    "\\left( \\begin{array}{c}h_1 \\\\ h_2 \\\\ h_3\\end{array}\\right) =0 $$\n",
    "\n",
    "This reasoning can be applied to all n corners in one image and the resulting equations can be stacked to yield one big matrix. The matrix $Q$ can be constructed using the observed corners. The least square solution for $H$ can be found with the **Singular Value Decomposition (SVD)** ([Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition#Solving_homogeneous_linear_equations)).\n",
    "$$ \\underbrace{\\left( \\begin{array}{ccc}\n",
    "P_1^T & 0^T & -u_1P_1^T \\\\\n",
    "0^T & P_1^T & -v_1P_1^T \\\\ \n",
    "P_2^T & 0^T & -u_2P_2^T \\\\\n",
    "0^T & P_2^T & -v_2P_2^T \\\\\n",
    "\\vdots & \\vdots & \\vdots \\\\\n",
    "P_n^T & 0^T & -u_nP_n^T \\\\\n",
    "0^T & P_n^T & -v_nP_n^T \\end{array} \\right)}_{Q\\in\\mathbb{R}^{2n \\times 9}} \\left( \\begin{array}{c}h_1 \\\\ h_2 \\\\ h_3\\end{array}\\right) = Q \\cdot H' = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-chase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.11077081e-03 1.46102659e-04 2.71518127e-01]\n",
      " [3.97549542e-04 7.06366424e-03 9.62380051e-01]\n",
      " [3.58815984e-07 2.88318579e-07 1.38076544e-03]]\n",
      "Average perspective projection equation error: 1.0310488434658875 px\n"
     ]
    }
   ],
   "source": [
    "def calculate_H_DLT(corners, chessboard_positions):\n",
    "    n = corners.shape[0]\n",
    "    \n",
    "    P = chessboard_positions.copy();\n",
    "    P[:,2] = 1\n",
    "    \n",
    "    Q = np.zeros((2*n, 9))\n",
    "    Q[::2,0:3] = P\n",
    "    Q[1::2,3:6] = P\n",
    "    Q[::2,6:9] = -(P.T * corners[:,0]).T\n",
    "    Q[1::2,6:9] = -(P.T * corners[:,1]).T\n",
    "    \n",
    "    U, D, Vh = np.linalg.svd(Q, full_matrices=True)\n",
    "    # Row in vh that is associated with the smalles singular value gives solution\n",
    "    H_tilde = Vh[-1,:]\n",
    "    H = H_tilde.reshape(3,3)\n",
    "    return H\n",
    "    \n",
    "def test_H_DLT():\n",
    "    H_0 = calculate_H_DLT(image_corners[0], chessboard_positions)\n",
    "    print(H_0)  \n",
    "    \n",
    "    # Check that perspective projection equation is approximately fulfilled\n",
    "    P = chessboard_positions.copy();\n",
    "    P[:,2] = 1\n",
    "    \n",
    "    pixels_h = H_0 @ P.T;\n",
    "    pixels = pixels_h[0:2,:]/pixels_h[2,:]\n",
    "    \n",
    "    print(\"Average perspective projection equation error:\", \n",
    "          np.average(np.linalg.norm(pixels.T - image_corners[0], axis=1)), 'px')\n",
    "\n",
    "test_H_DLT()\n",
    "\n",
    "Hs = np.array([calculate_H_DLT(image_corners[i], chessboard_positions) for i in range(image_corners.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-makeup",
   "metadata": {},
   "source": [
    "We can now compute the homography $H_i$ for each image $i$. Note that the scale of $H$ can be arbitrary chosen beause the homogeneous equation $Q\\cdot sH'=0$ is satisfied for any $s\\in\\mathbb{R}$. In the next step we need to extract the camera matrix from all matrices $H_i$. We will use the constraints imposed by the rotation matrix and rewrite the vectors $r_1$ and $r_2$ using $H$ and $K$ (and an unknown scale factor $s$). The vectors $g_j$ are the column vectors of $H$ and are not to be confused with the row vectors $h_j$ used in the derivation of $H$.\n",
    "$$ H = \\left[ \\begin{array}{c c c} g_1 & g_2 & g_3 \\end{array}\\right] = \n",
    "sK\\left[ \\begin{array}{c c c} r_1 & r_2 & t \\end{array}\\right] \\Rightarrow \\left\\{\n",
    "\\begin{array}{l l} r_1 = s^{-1} K^{-1}g_1 \\\\ r_2 = s^{-1} K^{-1}g_2 \\end{array}\\right.$$\n",
    "The constraints on the rotation matrix impose that $r_1$ and $r_2$ are orthonormal which translates to:\n",
    "$$ \n",
    "\\begin{align}\n",
    "    r_1 \\cdot r_2 = 0 & \\Rightarrow g_1^TK^{-T}K^{-1}g_2 = 0 \\\\\n",
    "    r_1 \\cdot r_1 = 1 = r_2 \\cdot r_2 & \\Rightarrow g_1^TK^{-T}K^{-1}g_1 = g_2^TK^{-T}K^{-1}g_2.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We define the symmetric matrix $B$ which is defined by the vector $b'= \\left( \\begin{array}{llllll}\n",
    "    b_{11} & b_{12} & b_{22} & b_{13}  & b_{23} & b_{33} \\end{array}\\right)^T$ as\n",
    "$$ B = \\left( \\begin{array}{ccc}\n",
    "    b_{11} & b_{12} & b_{13}\\\\ b_{12} & b_{22} & b_{23} \\\\ b_{13} & b_{23} & b_{33}\n",
    "\\end{array}\\right)  = K^{-T}K^{-1}.$$\n",
    "Using this noteation, the constraints can be rewritten as linear function of $b'$:\n",
    "$$ g_i^T B g_j = v_{ij}^Tb' $$\n",
    "with \n",
    "$$ v_{ij} = \\left( \\begin{array}{llllll} h_{1,i}h_{1,j} & h_{1,i}h_{2,j}+h_{2,i}h_{1,j} & h_{2,i}h_{2,j} & h_{3,i}h_{1,j}+h_{1,i}h_{3,j} & h_{3,i}h_{2,j}+h_{2,i}h_{3,j} & h_{3,i}h_{3,j}\\end{array}\\right)^T.$$\n",
    "The corresponding two homogeneous equation in $b'$ are:\n",
    "$$ \\left( \\begin{array}{c}\n",
    "v_{12}^T \\\\ (v_{11}-v_{22})^T\n",
    "\\end{array}\\right) b' = 0. $$\n",
    "For each image $i$ we obtain two homogeneous equations that we stack to the matrix $V$. $b'$ is found again with the SVD.\n",
    "$$ Vb'=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ongoing-cricket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.58805673e-07 -9.94081941e-10 -2.60286939e-04]\n",
      " [-9.94081941e-10  2.51578496e-07 -1.86041522e-04]\n",
      " [-2.60286939e-04 -1.86041522e-04  9.99999949e-01]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_B_DLT(Hs):\n",
    "    def v(i, j, H):\n",
    "        # Note the 0 indexing in numpy\n",
    "        return np.array([H[0,i]*H[0,j], \n",
    "                         H[0,i]*H[1,j] + H[1,i]*H[0,j], \n",
    "                         H[1,i]*H[1,j], \n",
    "                         H[2,i]*H[0,j] + H[0,i]*H[2,j], \n",
    "                         H[2,i]*H[1,j] + H[1,i]*H[2,j],\n",
    "                         H[2,i]*H[2,j]])\n",
    "    \n",
    "    n = image_corners.shape[0]\n",
    "    V = np.zeros((2*n, 6))\n",
    "    \n",
    "    for i in range(n):\n",
    "        H = Hs[i,:,:]\n",
    "        V[2*i,:] = v(0, 1, H)\n",
    "        V[2*i+1,:] = v(0, 0, H) - v(1, 1, H)\n",
    "        \n",
    "    U, D, Vh = np.linalg.svd(V, full_matrices=True)\n",
    "    b = Vh[-1,:]\n",
    "    B = np.array([[b[0], b[1], b[3]],\n",
    "                  [b[1], b[2], b[4]],\n",
    "                  [b[3], b[4], b[5]]])\n",
    "    return B\n",
    "\n",
    "B = calculate_B_DLT(Hs)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-anderson",
   "metadata": {},
   "source": [
    "We can finally recover the calibration matrix $K$ from $B$ using the **Cholesky Factorization**. Keep in mind that we can only estimate $B$ up to a constant scale factor $\\alpha$. Therefore $B=\\alpha K^{-T}K^{-1}.$ We can safely ignore the scale factor at first and later rescale the obtained matrix $K$ so that $K_{33} =1$. Recall that the Cholesky Factorization factorizes a square matrix $A$ in the product of a lower-triangular matrix $L$ and a upper-triangular matrix $L^T$: $A=LL^T$. We apply this to $B$:\n",
    "$$ B = LL^T = K'^{-T}K'^{-1} \\Rightarrow K'=L^{-T} $$\n",
    "The camera matrix $K$ can now be recovered as:\n",
    "$$ K = \\frac{1}{K'_{33}}K' $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recognized-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial guess for the camera matrix\n",
      "[[1521.55    5.93 1008.58]\n",
      " [   0.   1543.26  743.48]\n",
      " [   0.      0.      1.  ]]\n",
      "Camera matrix obtained by OpenCV\n",
      "[[1534.97    0.   1010.01]\n",
      " [   0.   1558.1   747.42]\n",
      " [   0.      0.      1.  ]]\n"
     ]
    }
   ],
   "source": [
    "def B_to_K(B):\n",
    "    L = np.linalg.cholesky(B)\n",
    "    K_p = np.linalg.inv(L).T\n",
    "    K = K_p/K_p[2,2]\n",
    "    \n",
    "    # Due to numeric imprecisions elements below the diaglonal are note exactely 0. We set then to 0 to be consistent.\n",
    "    K[1,0] = K[2,0] = K[2,1] = 0\n",
    "    return K\n",
    "\n",
    "camera_mat = B_to_K(B)\n",
    "with np.printoptions(suppress=True, precision=2):\n",
    "    print(\"Our initial guess for the camera matrix\")\n",
    "    print(camera_mat)\n",
    "    print(\"Camera matrix obtained by OpenCV\")\n",
    "    print(camera_mat_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-participant",
   "metadata": {},
   "source": [
    "As we can see we are already pretty close to the OpenCV solution which is a good sign! :) \n",
    "We can now also estimate the transformation from world to camera for each image by reusing the homographies and formulas seen above:\n",
    "$$ \n",
    "\\begin{align}\n",
    "r_1 &= s^{-1} K^{-1}g_1 \\\\ \n",
    "r_2 &= s^{-1} K^{-1}g_2 \\\\\n",
    "r_3 &= r_1 \\times r_2\\\\\n",
    "t &= s^{-1} K^{-1}g_3\n",
    "\\end{align}$$\n",
    "where $s$ is a normalization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preliminary-andrews",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated transformation for first image:\n",
      "[[ 9.96562736e-01 -2.52520270e-02 -7.91673691e-02 -1.65520456e+02]\n",
      " [ 1.90406128e-02  9.97237340e-01 -6.65965427e-02 -9.34651453e+00]\n",
      " [ 8.06236187e-02  6.47833102e-02  9.94290386e-01  3.10249017e+02]]\n",
      "OpenCV transformation for first image:\n",
      "[[ 9.97177827e-01 -2.39115000e-02 -7.11661526e-02 -1.65802975e+02]\n",
      " [ 1.82067928e-02  9.96648114e-01 -7.97561917e-02 -1.00295945e+01]\n",
      " [ 7.28347019e-02  7.82353986e-02  9.94270752e-01  3.17260315e+02]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_transformations(Hs, K):\n",
    "    n = Hs.shape[0]\n",
    "    transformations = np.zeros((n, 3, 4))\n",
    "    for i in range(n):\n",
    "        H = Hs[i]\n",
    "        r1p = np.linalg.solve(K, H[:,0])\n",
    "        s = np.linalg.norm(r1p)\n",
    "        transformations[i,:,0] = r1p/s\n",
    "        transformations[i,:,1] = np.linalg.solve(K, H[:,1])/s\n",
    "        transformations[i,:,2] = np.cross(transformations[i,:,0], transformations[i,:,1])\n",
    "        transformations[i,:,3] = np.linalg.solve(K, H[:,2])/s\n",
    "    return transformations\n",
    "transformations = calculate_transformations(Hs, camera_mat)\n",
    "print(\"Calculated transformation for first image:\")\n",
    "print(transformations[0])\n",
    "print(\"OpenCV transformation for first image:\")\n",
    "print(transformations_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-cocktail",
   "metadata": {},
   "source": [
    "### Distortion Coefficients\n",
    "One could try to estimate the distortion coefficients by using a least squares approach as described in Zhang's paper to obtain an initial guess. This guess can then be refined with non-linear optimization. In practice however one can often skip this step and initialize the refinement with all distortion coefficients set to 0.\n",
    "We first need to define a function that maps from undistorted pixel coordinates to distorted coordinates (see notebook *Image Formation* for more detailed information):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominican-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distort(uv, K, distortion_coeffs):\n",
    "    alpha_u = K[0,0]; alpha_v = K[1,1]\n",
    "    u0 = K[0,2]; v0 = K[1,2]\n",
    "    k1 = distortion_coeffs[0]; k2 = distortion_coeffs[1]; k3 = distortion_coeffs[4]\n",
    "    p1 = distortion_coeffs[2]; p2 = distortion_coeffs[3];\n",
    "    \n",
    "    # Convert to nomalized coordinates\n",
    "    xy = (uv - np.array([u0, v0]))/np.array([alpha_u, alpha_v])\n",
    "    \n",
    "    r2 = np.sum(xy**2, axis = 1)\n",
    "    \n",
    "    xyd = (xy.T * (1 + k1*r2 + k2*r2**2 + k3*r2**3)).T    \n",
    "    \n",
    "    xyd[:,0] += 2*p1*xy[:,0]*xy[:,1] + p2*(r2 + 2*xy[:,0]**2)\n",
    "    xyd[:,1] += p1*(r2 + 2*xy[:,1]**2) + 2*p2*xy[:,0]*xy[:,1]\n",
    "\n",
    "    uvd = xyd * np.array([alpha_u, alpha_v]) + np.array([u0, v0])    \n",
    "    return uvd\n",
    "dist = np.zeros(dist_cv.shape) # We have not yet estimated the distortion, we set the initial value to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-impossible",
   "metadata": {},
   "source": [
    "#### Reprojection Error\n",
    "The reprojection error measures the euclidian distance in pixels between the observed corner position and the position obtained by projecting the point using $K$ and the distortion coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "formed-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reprojection error for OpenCV camera matrix and distortion:\n",
      "0.7053207197529432\n",
      "Average reprojection error for our camera matrix without distortion:\n",
      "1.0289743326987029\n"
     ]
    }
   ],
   "source": [
    "def reproject(world_coords, transformation, K, distortion_coeffs):\n",
    "    world_coords_h = np.ones((world_coords.shape[0], 4))\n",
    "    world_coords_h[:,:3] = world_coords\n",
    "    uv_h = (K @ transformation @ world_coords_h.T).T\n",
    "    uv = (uv_h[:,:2].T/uv_h[:,2]).T\n",
    "    uvd = distort(uv, K, distortion_coeffs)\n",
    "    return uvd\n",
    "    \n",
    "def reprojection_error(observed_pixel_coords, world_coords, transformation, K, distortion_coeffs):\n",
    "    error = np.linalg.norm(reproject(world_coords, transformation, K, distortion_coeffs) \n",
    "                           - observed_pixel_coords, axis=1)\n",
    "    return error\n",
    "\n",
    "def average_reprojection_error(image_corners, chessboard_positions, transformations, K, distortion_coeffs):\n",
    "    return np.average([np.average(reprojection_error(image_corners[i], chessboard_positions, \n",
    "                                                     transformations[i], K, distortion_coeffs))\n",
    "                       for i in range(image_corners.shape[0])])\n",
    "\n",
    "print(\"Average reprojection error for OpenCV camera matrix and distortion:\")\n",
    "print(average_reprojection_error(image_corners, chessboard_positions, transformations_cv, camera_mat_cv, dist_cv))\n",
    "print(\"Average reprojection error for our camera matrix without distortion:\")\n",
    "print(average_reprojection_error(image_corners, chessboard_positions, transformations, camera_mat, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-perception",
   "metadata": {},
   "source": [
    "We can see that the OpenCV solution has a lower reprojection error, which is normal because we have not yet estimated the distortion coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-desktop",
   "metadata": {},
   "source": [
    "### Non-Linear Optimization\n",
    "We will now minimize the sum of squared reprojection errors of all collected corners using non-linear optimization. This will allow to simultaneously estimate the distortion coefficient and refine the camera matrix and transformations. \n",
    "\n",
    "We need to be careful when dealing with rotations: there are different representations for a rotation as for example the rotation matrix or a rotation vector. A rotation matrix is easy to manipulate but has 9 parameters and internal constraints (orthonormal vectors). A rotation vector is less convenient to use but only has 3 independent parameters which is suitable for optimization! We will use OpenCV's [Rodrigues function](https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac) for converting between the representations.\n",
    "\n",
    "We assemble a large parameter vector which contains the camera intrinsics, the distortion coefficients and the rotation + translation parameters for each view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "killing-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_param_vec(K, dist_coeffs, transformations):\n",
    "    n_views = transformations.shape[0]\n",
    "    param_vec = np.array([K[0,0], # alpha_u\n",
    "                          K[1,1], # alpha_v\n",
    "                          K[0,2], # u_0\n",
    "                          K[1,2]])# v_0\n",
    "    param_vec = np.append(param_vec, dist_coeffs)\n",
    "    for i in range(n_views):\n",
    "        t = transformations[i,:,3]\n",
    "        r = cv2.Rodrigues(transformations[i,:,:3])[0].flatten()\n",
    "        param_vec = np.append(param_vec, [t,r])\n",
    "\n",
    "    return param_vec, n_views\n",
    "\n",
    "def from_param_vec(param_vec, n_views):\n",
    "    K = np.array([[param_vec[0], 0,            param_vec[2]],\n",
    "                  [0,            param_vec[1], param_vec[3]],\n",
    "                  [0,            0,            1]])\n",
    "    dist_coeffs = param_vec[4:9]\n",
    "    transformations = np.zeros((n_views, 3, 4))\n",
    "    for i in range(n_views):\n",
    "        start_idx = 9+6*i\n",
    "        t = param_vec[start_idx:start_idx+3]\n",
    "        r = param_vec[start_idx+3:start_idx+6]\n",
    "        transformations[i,:,3] = t\n",
    "        transformations[i,:,:3] = cv2.Rodrigues(r)[0]\n",
    "    return K, dist_coeffs, transformations\n",
    "\n",
    "param_vector, n_views = to_param_vec(camera_mat, dist, transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-original",
   "metadata": {},
   "source": [
    "The function that we minimize is the sum (over images and corners in this image) of squared reprojection errors and can be written as:\n",
    "\n",
    "$$ \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "K, D, T_{i=1:n} = \\argmin_{K, D, T_{i=1:n}} \\sum_{i=1}^{n} \\sum_{c=1}^{m} \\Vert p_{c_i} - \\pi(P_c, K, D, T_i) \\Vert_{2}^{2}\n",
    "$$\n",
    "where $p_{c_i}$ is the observed pixel position of corner $c$ in image $i$, and $\\pi(P_c, K, D, T_i)$ is the projection of the corner $P_c$ to image $i$.\n",
    "\n",
    "We will use [scipy's non-linear least-squares solver](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html) which expects an error vector as input. It will minimize the norm of this error vector which is equivalent to the minimization described above. The solver can be given additional information like the jacobian (obtained e.g. by automatic differentiation). In the scope of this notebook however, we will let the solver handle everything including the numeric differentiation for estimating the jacobian. And it just works! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ranking-wealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_fun(param_vec, image_corners):\n",
    "    K, dist_coeffs, transformations = from_param_vec(param_vec, n_views)\n",
    "    err_vec = np.zeros(n_views * 2 * image_corners.shape[1])\n",
    "    for i in range(n_views):\n",
    "        err_vec[i*2*image_corners.shape[1]:(i+1)*2*image_corners.shape[1]] = \\\n",
    "            (image_corners[i] - reproject(chessboard_positions, transformations[i], K, dist_coeffs)).flatten()\n",
    "    return err_vec\n",
    "opt_result = least_squares(error_fun, param_vector, args=[image_corners])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fatal-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pinhole model without distortion ***\n",
      "Average reprojection error:\n",
      "1.0289743326987029\n",
      "Norm of error vector:\n",
      "109.4528605914948\n",
      "Camera Matrix:\n",
      "[[1521.54575    5.92771 1008.57921]\n",
      " [   0.      1543.25769  743.4822 ]\n",
      " [   0.         0.         1.     ]]\n",
      "Distortion parameters:\n",
      "[0. 0. 0. 0. 0.]\n",
      "\n",
      "\n",
      "*** OpenCV ***\n",
      "Average reprojection error:\n",
      "0.7053207197529432\n",
      "Norm of error vector:\n",
      "33.8256688085948\n",
      "Camera Matrix:\n",
      "[[1534.96821    0.      1010.00742]\n",
      " [   0.      1558.10358  747.42189]\n",
      " [   0.         0.         1.     ]]\n",
      "Distortion parameters:\n",
      "[ 0.22128 -0.9994  -0.00002  0.00005  1.39581]\n",
      "\n",
      "\n",
      "*** Pinhole model with distortion ***\n",
      "Average reprojection error:\n",
      "0.7053207228409846\n",
      "Norm of error vector:\n",
      "33.82566880871311\n",
      "Camera Matrix:\n",
      "[[1534.96822    0.      1010.0073 ]\n",
      " [   0.      1558.10359  747.42198]\n",
      " [   0.         0.         1.     ]]\n",
      "Distortion parameters:\n",
      "[ 0.22128 -0.9994  -0.00002  0.00005  1.39581]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "camera_mat_opt, dist_opt, transformations_opt = from_param_vec(opt_result.x, n_views)\n",
    "\n",
    "with np.printoptions(suppress=True, precision=5):\n",
    "    def print_method_results(name, transformations, K, dist):\n",
    "        print(\"*** \"+ name +\" ***\")\n",
    "        print(\"Average reprojection error:\")\n",
    "        print(average_reprojection_error(image_corners, chessboard_positions, transformations, K, dist))\n",
    "        print(\"Norm of error vector:\")\n",
    "        print(np.linalg.norm(error_fun(to_param_vec(K, dist, transformations)[0], image_corners)))\n",
    "        print(\"Camera Matrix:\")\n",
    "        print(K)\n",
    "        print(\"Distortion parameters:\")\n",
    "        print(dist)\n",
    "        print(\"\\n\")\n",
    "    print_method_results(\"Pinhole model without distortion\", transformations, camera_mat, dist)\n",
    "    print_method_results(\"OpenCV\", transformations_cv, camera_mat_cv, dist_cv)\n",
    "    print_method_results(\"Pinhole model with distortion\", transformations_opt, camera_mat_opt, dist_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-privacy",
   "metadata": {},
   "source": [
    "We have successfully recreated the calibration obtained by OpenCV in a few lines of Python code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
